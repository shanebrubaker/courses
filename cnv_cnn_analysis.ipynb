{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN-Based Copy Number Variation (CNV) Analysis\n",
    "# Genetic Testing Pipeline\n",
    "\n",
    "This notebook demonstrates a complete pipeline for detecting copy number variations using convolutional neural networks.\n",
    "\n",
    "**Components:**\n",
    "1. Simulated genomic coverage data generation\n",
    "2. CNN architecture for CNV detection\n",
    "3. Model training and evaluation\n",
    "4. Comprehensive visualizations\n",
    "\n",
    "**Compatible with Apple Silicon Macs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import warnings\n",
    "import os\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create output directory\n",
    "OUTPUT_DIR = Path('./outputs')\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "print(f\"Output directory: {OUTPUT_DIR.absolute()}\")\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device configuration - Using CPU for stability\n",
    "# NOTE: MPS has known stability issues in PyTorch 2.0.1, especially with BatchNorm\n",
    "print(\"Available backends:\")\n",
    "print(f\"  MPS (Apple Silicon): {torch.backends.mps.is_available()}\")\n",
    "print(f\"  CUDA: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Force CPU for stability (uncomment below to try MPS again)\n",
    "device = torch.device('cpu')\n",
    "print(f\"\\nUsing: CPU (for stability)\")\n",
    "print(f\"  To use MPS, change device to torch.device('mps') in cell 1\")\n",
    "\n",
    "# Uncomment below to use MPS (may cause kernel restarts in PyTorch 2.0.1)\n",
    "# if torch.backends.mps.is_available():\n",
    "#     device = torch.device('mps')\n",
    "#     print(f\"Using Apple Silicon GPU (MPS)\")\n",
    "# elif torch.cuda.is_available():\n",
    "#     device = torch.device('cuda')\n",
    "#     print(f\"Using CUDA GPU\")\n",
    "# else:\n",
    "#     device = torch.device('cpu')\n",
    "#     print(f\"Using CPU\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simulated Genomic Coverage Data Generation\n",
    "\n",
    "We'll simulate read depth coverage data across genomic windows with three scenarios:\n",
    "- **Normal (Class 0)**: Diploid coverage (2 copies)\n",
    "- **Deletion (Class 1)**: Reduced coverage (1 copy or homozygous deletion)\n",
    "- **Duplication (Class 2)**: Increased coverage (3+ copies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_coverage_data(n_samples=1000, window_size=200, baseline_coverage=50):\n",
    "    \"\"\"\n",
    "    Generate simulated genomic coverage data for CNV detection.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_samples : int\n",
    "        Number of samples per class\n",
    "    window_size : int\n",
    "        Number of genomic windows (features)\n",
    "    baseline_coverage : float\n",
    "        Expected coverage for diploid regions\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X : numpy array of shape (n_samples*3, window_size, 1)\n",
    "        Coverage data\n",
    "    y : numpy array of shape (n_samples*3,)\n",
    "        Labels (0=normal, 1=deletion, 2=duplication)\n",
    "    \"\"\"\n",
    "    \n",
    "    X_normal = []\n",
    "    X_deletion = []\n",
    "    X_duplication = []\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        # Normal coverage (diploid, 2 copies)\n",
    "        normal = np.random.poisson(baseline_coverage, window_size)\n",
    "        # Add some biological noise (GC bias, mappability)\n",
    "        gc_bias = np.sin(np.linspace(0, 2*np.pi, window_size)) * 5\n",
    "        normal = normal + gc_bias + np.random.normal(0, 3, window_size)\n",
    "        X_normal.append(normal)\n",
    "        \n",
    "        # Deletion (hemizygous ~0.5x or homozygous deletion ~0x)\n",
    "        deletion_factor = np.random.uniform(0.1, 0.6)  # 10-60% of normal coverage\n",
    "        # Create a deletion region in the middle\n",
    "        deletion = np.random.poisson(baseline_coverage, window_size)\n",
    "        deletion = deletion + gc_bias + np.random.normal(0, 3, window_size)\n",
    "        deletion_start = window_size // 4\n",
    "        deletion_end = 3 * window_size // 4\n",
    "        deletion[deletion_start:deletion_end] *= deletion_factor\n",
    "        X_deletion.append(deletion)\n",
    "        \n",
    "        # Duplication (3+ copies)\n",
    "        duplication_factor = np.random.uniform(1.4, 2.5)  # 140-250% of normal coverage\n",
    "        # Create a duplication region in the middle\n",
    "        duplication = np.random.poisson(baseline_coverage, window_size)\n",
    "        duplication = duplication + gc_bias + np.random.normal(0, 3, window_size)\n",
    "        dup_start = window_size // 4\n",
    "        dup_end = 3 * window_size // 4\n",
    "        duplication[dup_start:dup_end] *= duplication_factor\n",
    "        X_duplication.append(duplication)\n",
    "    \n",
    "    # Combine all data\n",
    "    X_normal = np.array(X_normal)\n",
    "    X_deletion = np.array(X_deletion)\n",
    "    X_duplication = np.array(X_duplication)\n",
    "    \n",
    "    X = np.vstack([X_normal, X_deletion, X_duplication])\n",
    "    y = np.hstack([np.zeros(n_samples), np.ones(n_samples), np.ones(n_samples)*2])\n",
    "    \n",
    "    # Normalize coverage data\n",
    "    X = X.reshape(-1, window_size, 1).astype(np.float32)\n",
    "    \n",
    "    # Shuffle the data\n",
    "    indices = np.random.permutation(len(X))\n",
    "    X = X[indices]\n",
    "    y = y[indices].astype(np.int64)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate training and test data\n",
    "print(\"Generating training data...\")\n",
    "X_train, y_train = generate_coverage_data(n_samples=800, window_size=200)\n",
    "\n",
    "print(\"Generating test data...\")\n",
    "X_test, y_test = generate_coverage_data(n_samples=200, window_size=200)\n",
    "\n",
    "print(f\"\\nTraining set shape: {X_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")\n",
    "\n",
    "# Class distribution\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "print(f\"  Normal (0): {counts[0]} samples\")\n",
    "print(f\"  Deletion (1): {counts[1]} samples\")\n",
    "print(f\"  Duplication (2): {counts[2]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize example coverage profiles\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
    "\n",
    "class_names = ['Normal', 'Deletion', 'Duplication']\n",
    "colors = ['#2ecc71', '#e74c3c', '#3498db']\n",
    "\n",
    "for class_idx, (ax, name, color) in enumerate(zip(axes, class_names, colors)):\n",
    "    # Find an example of this class\n",
    "    example_idx = np.where(y_train == class_idx)[0][0]\n",
    "    coverage = X_train[example_idx].flatten()\n",
    "    \n",
    "    ax.plot(coverage, color=color, linewidth=1.5, alpha=0.8)\n",
    "    ax.fill_between(range(len(coverage)), coverage, alpha=0.3, color=color)\n",
    "    ax.set_title(f'{name} Coverage Profile', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Genomic Window', fontsize=12)\n",
    "    ax.set_ylabel('Read Depth', fontsize=12)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add shaded region for CNV location\n",
    "    if class_idx > 0:  # Deletion or Duplication\n",
    "        ax.axvspan(50, 150, alpha=0.2, color='gray', label='CNV Region')\n",
    "        ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'coverage_profiles.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Coverage profiles saved to: {OUTPUT_DIR / 'coverage_profiles.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CNN Architecture for CNV Detection\n",
    "\n",
    "Our CNN architecture:\n",
    "- **Conv1D layers**: Extract local patterns in coverage data\n",
    "- **Batch Normalization**: Stabilize training\n",
    "- **MaxPooling**: Reduce dimensionality\n",
    "- **Dropout**: Prevent overfitting\n",
    "- **Fully Connected layers**: Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNV_CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Neural Network for Copy Number Variation detection.\n",
    "    \n",
    "    Architecture:\n",
    "    - 3 Convolutional blocks (Conv1D + BatchNorm + ReLU + MaxPool)\n",
    "    - 2 Fully connected layers\n",
    "    - Dropout for regularization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=200, num_classes=3):\n",
    "        super(CNV_CNN, self).__init__()\n",
    "        \n",
    "        # Convolutional Block 1\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=7, padding=3)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # Convolutional Block 2\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=5, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # Convolutional Block 3\n",
    "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # Calculate size after convolutions and pooling\n",
    "        # input_size=200 -> after 3 maxpool(2): 200/2/2/2 = 25\n",
    "        self.fc_input_size = 128 * 25\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self.fc_input_size, 256)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, 200, 1) -> need (batch, 1, 200)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        # Conv Block 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # Conv Block 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # Conv Block 3\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "model = CNV_CNN(input_size=200, num_classes=3).to(device)\n",
    "\n",
    "print(\"CNN Model Architecture:\")\n",
    "print(\"=\"*60)\n",
    "print(model)\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CNN Architecture\n",
    "def visualize_cnn_architecture():\n",
    "    \"\"\"\n",
    "    Create a visual representation of the CNN architecture.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(16, 10))\n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_ylim(0, 10)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Define layer positions and properties\n",
    "    layers = [\n",
    "        {'name': 'Input\\n(200x1)', 'x': 0.5, 'y': 5, 'width': 0.3, 'height': 4, 'color': '#95a5a6'},\n",
    "        {'name': 'Conv1D(32)\\n7x1', 'x': 1.5, 'y': 5, 'width': 0.4, 'height': 4, 'color': '#3498db'},\n",
    "        {'name': 'BatchNorm\\n+ ReLU', 'x': 2.3, 'y': 5, 'width': 0.3, 'height': 4, 'color': '#9b59b6'},\n",
    "        {'name': 'MaxPool(2)', 'x': 3.0, 'y': 5, 'width': 0.3, 'height': 3.5, 'color': '#e74c3c'},\n",
    "        \n",
    "        {'name': 'Conv1D(64)\\n5x1', 'x': 3.8, 'y': 5, 'width': 0.4, 'height': 3.5, 'color': '#3498db'},\n",
    "        {'name': 'BatchNorm\\n+ ReLU', 'x': 4.6, 'y': 5, 'width': 0.3, 'height': 3.5, 'color': '#9b59b6'},\n",
    "        {'name': 'MaxPool(2)', 'x': 5.3, 'y': 5, 'width': 0.3, 'height': 3, 'color': '#e74c3c'},\n",
    "        \n",
    "        {'name': 'Conv1D(128)\\n3x1', 'x': 6.1, 'y': 5, 'width': 0.4, 'height': 3, 'color': '#3498db'},\n",
    "        {'name': 'BatchNorm\\n+ ReLU', 'x': 6.9, 'y': 5, 'width': 0.3, 'height': 3, 'color': '#9b59b6'},\n",
    "        {'name': 'MaxPool(2)', 'x': 7.6, 'y': 5, 'width': 0.3, 'height': 2.5, 'color': '#e74c3c'},\n",
    "        \n",
    "        {'name': 'Flatten', 'x': 8.3, 'y': 5, 'width': 0.2, 'height': 2.5, 'color': '#f39c12'},\n",
    "        {'name': 'FC(256)\\n+ Dropout', 'x': 8.8, 'y': 5, 'width': 0.3, 'height': 2, 'color': '#2ecc71'},\n",
    "        {'name': 'FC(128)\\n+ Dropout', 'x': 9.4, 'y': 5, 'width': 0.3, 'height': 1.5, 'color': '#2ecc71'},\n",
    "        {'name': 'Output\\n(3 classes)', 'x': 10.0, 'y': 5, 'width': 0.3, 'height': 1, 'color': '#e67e22'},\n",
    "    ]\n",
    "    \n",
    "    # Draw layers\n",
    "    for i, layer in enumerate(layers):\n",
    "        # Draw rectangle\n",
    "        rect = plt.Rectangle(\n",
    "            (layer['x'] - layer['width']/2, layer['y'] - layer['height']/2),\n",
    "            layer['width'], layer['height'],\n",
    "            facecolor=layer['color'], edgecolor='black', linewidth=2, alpha=0.7\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Add text\n",
    "        ax.text(layer['x'], layer['y'], layer['name'],\n",
    "                ha='center', va='center', fontsize=9, fontweight='bold',\n",
    "                color='white', bbox=dict(boxstyle='round', facecolor='black', alpha=0.5))\n",
    "        \n",
    "        # Draw arrows between layers\n",
    "        if i < len(layers) - 1:\n",
    "            next_layer = layers[i + 1]\n",
    "            ax.arrow(layer['x'] + layer['width']/2 + 0.05, layer['y'],\n",
    "                    next_layer['x'] - next_layer['width']/2 - layer['x'] - layer['width']/2 - 0.1, 0,\n",
    "                    head_width=0.15, head_length=0.08, fc='black', ec='black', linewidth=2)\n",
    "    \n",
    "    # Add title and legend\n",
    "    ax.text(5.5, 9.5, 'CNN Architecture for CNV Detection',\n",
    "            ha='center', fontsize=18, fontweight='bold')\n",
    "    \n",
    "    # Add legend\n",
    "    legend_elements = [\n",
    "        plt.Rectangle((0, 0), 1, 1, fc='#3498db', alpha=0.7, label='Convolutional'),\n",
    "        plt.Rectangle((0, 0), 1, 1, fc='#9b59b6', alpha=0.7, label='Normalization'),\n",
    "        plt.Rectangle((0, 0), 1, 1, fc='#e74c3c', alpha=0.7, label='Pooling'),\n",
    "        plt.Rectangle((0, 0), 1, 1, fc='#2ecc71', alpha=0.7, label='Fully Connected'),\n",
    "        plt.Rectangle((0, 0), 1, 1, fc='#f39c12', alpha=0.7, label='Flatten'),\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='lower center', ncol=5, fontsize=10, frameon=True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR / 'cnn_architecture.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.show()\n",
    "\n",
    "visualize_cnn_architecture()\n",
    "print(f\"✓ CNN architecture saved to: {OUTPUT_DIR / 'cnn_architecture.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data loaders\n",
    "train_dataset = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "test_dataset = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Batch size: 32\")\n",
    "print(f\"  Learning rate: 0.001\")\n",
    "print(f\"  Optimizer: Adam\")\n",
    "print(f\"  Loss function: CrossEntropyLoss\")\n",
    "print(f\"  Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with MPS stability fixes\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, labels) in enumerate(loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Move to CPU and detach before accumulating to avoid MPS memory issues\n",
    "        running_loss += loss.detach().cpu().item()\n",
    "        _, predicted = torch.max(outputs.detach(), 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted.cpu() == labels.cpu()).sum().item()\n",
    "        \n",
    "        # Clear MPS cache periodically to prevent memory buildup\n",
    "        if device.type == 'mps' and batch_idx % 10 == 0:\n",
    "            torch.mps.empty_cache()\n",
    "    \n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, labels) in enumerate(loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Move to CPU before accumulating\n",
    "            running_loss += loss.cpu().item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted.cpu() == labels.cpu()).sum().item()\n",
    "            \n",
    "            # Clear MPS cache periodically\n",
    "            if device.type == 'mps' and batch_idx % 10 == 0:\n",
    "                torch.mps.empty_cache()\n",
    "    \n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "\n",
    "print(\"\\nStarting training...\\n\")\n",
    "print(f\"{'Epoch':<6} {'Train Loss':<12} {'Train Acc':<12} {'Val Loss':<12} {'Val Acc':<12}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), OUTPUT_DIR / 'best_cnv_model.pth')\n",
    "    \n",
    "    # Clear cache after each epoch\n",
    "    if device.type == 'mps':\n",
    "        torch.mps.empty_cache()\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"{epoch+1:<6} {train_loss:<12.4f} {train_acc:<12.2f} {val_loss:<12.4f} {val_acc:<12.2f}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTraining completed!\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"Model saved to: {OUTPUT_DIR / 'best_cnv_model.pth'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Loss plot\n",
    "ax1.plot(train_losses, label='Training Loss', linewidth=2, color='#3498db')\n",
    "ax1.plot(val_losses, label='Validation Loss', linewidth=2, color='#e74c3c')\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "ax2.plot(train_accs, label='Training Accuracy', linewidth=2, color='#2ecc71')\n",
    "ax2.plot(val_accs, label='Validation Accuracy', linewidth=2, color='#f39c12')\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax2.set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Training history saved to: {OUTPUT_DIR / 'training_history.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation and Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load(OUTPUT_DIR / 'best_cnv_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Get predictions\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "all_probs = np.array(all_probs)\n",
    "\n",
    "# Classification report\n",
    "class_names = ['Normal', 'Deletion', 'Duplication']\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(all_labels, all_preds, target_names=class_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Raw counts\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names,\n",
    "            yticklabels=class_names, ax=ax1, cbar_kws={'label': 'Count'})\n",
    "ax1.set_title('Confusion Matrix (Counts)', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('True Label', fontsize=12)\n",
    "ax1.set_xlabel('Predicted Label', fontsize=12)\n",
    "\n",
    "# Normalized\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.3f', cmap='RdYlGn', xticklabels=class_names,\n",
    "            yticklabels=class_names, ax=ax2, cbar_kws={'label': 'Proportion'})\n",
    "ax2.set_title('Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('True Label', fontsize=12)\n",
    "ax2.set_xlabel('Predicted Label', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Confusion matrix saved to: {OUTPUT_DIR / 'confusion_matrix.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves for multi-class classification\n",
    "y_test_bin = label_binarize(all_labels, classes=[0, 1, 2])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "colors = ['#2ecc71', '#e74c3c', '#3498db']\n",
    "for i, (class_name, color) in enumerate(zip(class_names, colors)):\n",
    "    fpr, tpr, _ = roc_curve(y_test_bin[:, i], all_probs[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    ax.plot(fpr, tpr, color=color, lw=2, \n",
    "            label=f'{class_name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "ax.set_title('ROC Curves for CNV Classification', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='lower right', fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ ROC curves saved to: {OUTPUT_DIR / 'roc_curves.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction examples with coverage profiles\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 12))\n",
    "\n",
    "for class_idx in range(3):\n",
    "    # Find examples of each class\n",
    "    class_indices = np.where(y_test == class_idx)[0][:3]\n",
    "    \n",
    "    for i, idx in enumerate(class_indices):\n",
    "        ax = axes[class_idx, i]\n",
    "        coverage = X_test[idx].flatten()\n",
    "        \n",
    "        # Get prediction\n",
    "        pred_class = all_preds[idx]\n",
    "        pred_prob = all_probs[idx, pred_class] * 100\n",
    "        \n",
    "        # Plot coverage\n",
    "        color = colors[class_idx]\n",
    "        ax.plot(coverage, color=color, linewidth=1.5, alpha=0.8)\n",
    "        ax.fill_between(range(len(coverage)), coverage, alpha=0.3, color=color)\n",
    "        \n",
    "        # Title with prediction\n",
    "        correct = \"✓\" if pred_class == class_idx else \"✗\"\n",
    "        title = f\"{correct} True: {class_names[class_idx]}\\nPred: {class_names[pred_class]} ({pred_prob:.1f}%)\"\n",
    "        ax.set_title(title, fontsize=11, fontweight='bold')\n",
    "        \n",
    "        ax.set_xlabel('Genomic Window', fontsize=10)\n",
    "        ax.set_ylabel('Read Depth', fontsize=10)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Highlight CNV region for deletions and duplications\n",
    "        if class_idx > 0:\n",
    "            ax.axvspan(50, 150, alpha=0.15, color='gray')\n",
    "\n",
    "plt.suptitle('CNV Prediction Examples with Coverage Profiles', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'prediction_examples.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Prediction examples saved to: {OUTPUT_DIR / 'prediction_examples.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance via layer activations\n",
    "def get_conv_activations(model, x, layer_name='conv1'):\n",
    "    \"\"\"\n",
    "    Extract activations from a specific convolutional layer.\n",
    "    \"\"\"\n",
    "    activations = {}\n",
    "    \n",
    "    def hook_fn(module, input, output):\n",
    "        activations['output'] = output.detach()\n",
    "    \n",
    "    # Register hook\n",
    "    if layer_name == 'conv1':\n",
    "        hook = model.conv1.register_forward_hook(hook_fn)\n",
    "    elif layer_name == 'conv2':\n",
    "        hook = model.conv2.register_forward_hook(hook_fn)\n",
    "    elif layer_name == 'conv3':\n",
    "        hook = model.conv3.register_forward_hook(hook_fn)\n",
    "    \n",
    "    # Forward pass\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _ = model(x)\n",
    "    \n",
    "    hook.remove()\n",
    "    return activations['output']\n",
    "\n",
    "# Visualize activations for one example of each class\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 12))\n",
    "\n",
    "layer_names = ['conv1', 'conv2', 'conv3']\n",
    "\n",
    "for class_idx in range(3):\n",
    "    # Get one example\n",
    "    example_idx = np.where(y_test == class_idx)[0][0]\n",
    "    example = torch.from_numpy(X_test[example_idx:example_idx+1]).to(device)\n",
    "    \n",
    "    for layer_idx, layer_name in enumerate(layer_names):\n",
    "        ax = axes[class_idx, layer_idx]\n",
    "        \n",
    "        # Get activations\n",
    "        activations = get_conv_activations(model, example, layer_name)\n",
    "        act_np = activations.cpu().numpy()[0]  # Shape: (channels, length)\n",
    "        \n",
    "        # Plot heatmap of activations\n",
    "        im = ax.imshow(act_np[:16], aspect='auto', cmap='viridis', interpolation='nearest')\n",
    "        ax.set_title(f'{class_names[class_idx]} - {layer_name.upper()}', \n",
    "                    fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel('Position', fontsize=10)\n",
    "        ax.set_ylabel('Filter', fontsize=10)\n",
    "        plt.colorbar(im, ax=ax, label='Activation')\n",
    "\n",
    "plt.suptitle('CNN Layer Activations for Different CNV Types', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'layer_activations.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Layer activations saved to: {OUTPUT_DIR / 'layer_activations.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate per-class metrics\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precision, recall, f1, support = precision_recall_fscore_support(all_labels, all_preds)\n",
    "\n",
    "# Create summary table\n",
    "summary_data = {\n",
    "    'Class': class_names,\n",
    "    'Precision': [f\"{p:.4f}\" for p in precision],\n",
    "    'Recall': [f\"{r:.4f}\" for r in recall],\n",
    "    'F1-Score': [f\"{f:.4f}\" for f in f1],\n",
    "    'Support': support.astype(int)\n",
    "}\n",
    "\n",
    "# Overall accuracy\n",
    "accuracy = np.mean(all_preds == all_labels) * 100\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CNV DETECTION MODEL - FINAL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nOverall Accuracy: {accuracy:.2f}%\\n\")\n",
    "print(f\"{'Class':<15} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<10}\")\n",
    "print(\"-\"*70)\n",
    "for i in range(len(class_names)):\n",
    "    print(f\"{summary_data['Class'][i]:<15} {summary_data['Precision'][i]:<12} \"\n",
    "          f\"{summary_data['Recall'][i]:<12} {summary_data['F1-Score'][i]:<12} \"\n",
    "          f\"{summary_data['Support'][i]:<10}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Additional metrics\n",
    "print(f\"\\nModel Parameters: {total_params:,}\")\n",
    "print(f\"Training Epochs: {num_epochs}\")\n",
    "print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"Final Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"All visualizations have been saved to: {OUTPUT_DIR.absolute()}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clinical Interpretation Guidelines\n",
    "\n",
    "### Model Output Interpretation:\n",
    "\n",
    "1. **Normal (Class 0)**: Diploid copy number (2 copies)\n",
    "   - Expected in regions without structural variants\n",
    "   - Confidence >95% recommended for clinical reporting\n",
    "\n",
    "2. **Deletion (Class 1)**: Copy number loss\n",
    "   - Hemizygous deletion (1 copy) or homozygous deletion (0 copies)\n",
    "   - May indicate pathogenic variants if in disease-associated regions\n",
    "   - Recommend orthogonal validation (e.g., MLPA, qPCR)\n",
    "\n",
    "3. **Duplication (Class 2)**: Copy number gain\n",
    "   - Heterozygous duplication (3 copies) or higher amplification\n",
    "   - Clinical significance depends on gene dosage sensitivity\n",
    "   - Consider breakpoint mapping for precise characterization\n",
    "\n",
    "### Quality Control Recommendations:\n",
    "- Minimum coverage: 30x for reliable CNV calling\n",
    "- GC content normalization applied during preprocessing\n",
    "- Manual review recommended for predictions with <90% confidence\n",
    "- Consider batch effects in multi-sample analyses\n",
    "\n",
    "### Limitations:\n",
    "- Model trained on simulated data; validation on real clinical samples required\n",
    "- Resolution limited to window size (200 bins in this implementation)\n",
    "- Complex rearrangements may require additional analysis\n",
    "- Balanced sex chromosomes and common variants not specifically modeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## End of Notebook ##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
